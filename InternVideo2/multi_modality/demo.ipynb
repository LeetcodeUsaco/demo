{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/root/IV2/InternVideo2/multi_modality/models/backbones/internvideo2/internvl_clip_vision.py:140: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n",
      "/root/IV2/InternVideo2/multi_modality/models/backbones/internvideo2/internvideo2.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n",
      "/usr/local/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/root/IV2/InternVideo2/multi_modality/models/backbones/internvideo2/internvideo2_clip_vision.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'deepspeed'\n",
      "deepspeed is not installed!!!\n"
     ]
    }
   ],
   "source": [
    "from os import system as run\n",
    "import os\n",
    "\n",
    "os.chdir(\"/root/IV2/InternVideo2/multi_modality\")\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import numpy as np\n",
    "import io\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "\n",
    "from demo.config import (Config,\n",
    "                    eval_dict_leaf)\n",
    "\n",
    "from demo.utils import (retrieve_text,\n",
    "                  _frame_from_video,\n",
    "                  setup_internvideo2)\n",
    "\n",
    "from iv2_utils.iv2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded vision checkpoint to /root/.cache/huggingface/hub/models--OpenGVLab--InternVideo2-Stage2_6B-224p-f4/snapshots/03fa261db1acaa6c8ce4bf976e3041692ced779e/internvideo2-s2_6b-224p-f4.pt\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from transformers import AutoTokenizer\n",
    "from models.backbones.bert.tokenization_bert import BertTokenizer\n",
    "\n",
    "TOKEN = \"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\"\n",
    "\n",
    "MODEL_NAME = \"6B\"\n",
    "\n",
    "vision_ckpt_path = hf_hub_download(repo_id=\"OpenGVLab/InternVideo2-Stage2_6B-224p-f4\", filename=\"internvideo2-s2_6b-224p-f4.pt\")\n",
    "\n",
    "print(f\"Downloaded vision checkpoint to {vision_ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config.from_file('scripts/pretraining/stage2/6B/config.py')\n",
    "config = eval_dict_leaf(config)\n",
    "\n",
    "config.model.vision_ckpt_path = vision_ckpt_path\n",
    "config.model.vision_encoder.pretrained = vision_ckpt_path\n",
    "config.pretrained_path = vision_ckpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_state_dict: _IncompatibleKeys(missing_keys=[], unexpected_keys=['temp', 'av_concat_vision_proj.0.weight', 'av_concat_vision_proj.0.bias', 'av_concat_vision_proj.1.weight', 'av_concat_vision_proj.1.bias', 'audio_encoder.post_extract_proj.weight', 'audio_encoder.post_extract_proj.bias', 'audio_encoder.patch_embedding.weight', 'audio_encoder.encoder.pos_conv.0.bias', 'audio_encoder.encoder.pos_conv.0.weight_g', 'audio_encoder.encoder.pos_conv.0.weight_v', 'audio_encoder.encoder.layers.0.self_attn.grep_a', 'audio_encoder.encoder.layers.0.self_attn.relative_attention_bias.weight', 'audio_encoder.encoder.layers.0.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.0.self_attn.k_proj.bias', 'audio_encoder.encoder.layers.0.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.0.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.0.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.0.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.0.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.0.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.0.self_attn.grep_linear.weight', 'audio_encoder.encoder.layers.0.self_attn.grep_linear.bias', 'audio_encoder.encoder.layers.0.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.0.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.0.fc1.weight', 'audio_encoder.encoder.layers.0.fc1.bias', 'audio_encoder.encoder.layers.0.fc2.weight', 'audio_encoder.encoder.layers.0.fc2.bias', 'audio_encoder.encoder.layers.0.final_layer_norm.weight', 'audio_encoder.encoder.layers.0.final_layer_norm.bias', 'audio_encoder.encoder.layers.1.self_attn.grep_a', 'audio_encoder.encoder.layers.1.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.1.self_attn.k_proj.bias', 'audio_encoder.encoder.layers.1.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.1.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.1.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.1.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.1.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.1.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.1.self_attn.grep_linear.weight', 'audio_encoder.encoder.layers.1.self_attn.grep_linear.bias', 'audio_encoder.encoder.layers.1.self_attn.relative_attention_bias.weight', 'audio_encoder.encoder.layers.1.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.1.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.1.fc1.weight', 'audio_encoder.encoder.layers.1.fc1.bias', 'audio_encoder.encoder.layers.1.fc2.weight', 'audio_encoder.encoder.layers.1.fc2.bias', 'audio_encoder.encoder.layers.1.final_layer_norm.weight', 'audio_encoder.encoder.layers.1.final_layer_norm.bias', 'audio_encoder.encoder.layers.2.self_attn.grep_a', 'audio_encoder.encoder.layers.2.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.2.self_attn.k_proj.bias', 'audio_encoder.encoder.layers.2.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.2.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.2.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.2.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.2.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.2.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.2.self_attn.grep_linear.weight', 'audio_encoder.encoder.layers.2.self_attn.grep_linear.bias', 'audio_encoder.encoder.layers.2.self_attn.relative_attention_bias.weight', 'audio_encoder.encoder.layers.2.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.2.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.2.fc1.weight', 'audio_encoder.encoder.layers.2.fc1.bias', 'audio_encoder.encoder.layers.2.fc2.weight', 'audio_encoder.encoder.layers.2.fc2.bias', 'audio_encoder.encoder.layers.2.final_layer_norm.weight', 'audio_encoder.encoder.layers.2.final_layer_norm.bias', 'audio_encoder.encoder.layers.3.self_attn.grep_a', 'audio_encoder.encoder.layers.3.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.3.self_attn.k_proj.bias', 'audio_encoder.encoder.layers.3.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.3.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.3.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.3.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.3.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.3.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.3.self_attn.grep_linear.weight', 'audio_encoder.encoder.layers.3.self_attn.grep_linear.bias', 'audio_encoder.encoder.layers.3.self_attn.relative_attention_bias.weight', 'audio_encoder.encoder.layers.3.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.3.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.3.fc1.weight', 'audio_encoder.encoder.layers.3.fc1.bias', 'audio_encoder.encoder.layers.3.fc2.weight', 'audio_encoder.encoder.layers.3.fc2.bias', 'audio_encoder.encoder.layers.3.final_layer_norm.weight', 'audio_encoder.encoder.layers.3.final_layer_norm.bias', 'audio_encoder.encoder.layers.4.self_attn.grep_a', 'audio_encoder.encoder.layers.4.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.4.self_attn.k_proj.bias', 'audio_encoder.encoder.layers.4.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.4.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.4.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.4.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.4.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.4.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.4.self_attn.grep_linear.weight', 'audio_encoder.encoder.layers.4.self_attn.grep_linear.bias', 'audio_encoder.encoder.layers.4.self_attn.relative_attention_bias.weight', 'audio_encoder.encoder.layers.4.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.4.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.4.fc1.weight', 'audio_encoder.encoder.layers.4.fc1.bias', 'audio_encoder.encoder.layers.4.fc2.weight', 'audio_encoder.encoder.layers.4.fc2.bias', 'audio_encoder.encoder.layers.4.final_layer_norm.weight', 'audio_encoder.encoder.layers.4.final_layer_norm.bias', 'audio_encoder.encoder.layers.5.self_attn.grep_a', 'audio_encoder.encoder.layers.5.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.5.self_attn.k_proj.bias', 'audio_encoder.encoder.layers.5.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.5.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.5.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.5.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.5.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.5.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.5.self_attn.grep_linear.weight', 'audio_encoder.encoder.layers.5.self_attn.grep_linear.bias', 'audio_encoder.encoder.layers.5.self_attn.relative_attention_bias.weight', 'audio_encoder.encoder.layers.5.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.5.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.5.fc1.weight', 'audio_encoder.encoder.layers.5.fc1.bias', 'audio_encoder.encoder.layers.5.fc2.weight', 'audio_encoder.encoder.layers.5.fc2.bias', 'audio_encoder.encoder.layers.5.final_layer_norm.weight', 'audio_encoder.encoder.layers.5.final_layer_norm.bias', 'audio_encoder.encoder.layers.6.self_attn.grep_a', 'audio_encoder.encoder.layers.6.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.6.self_attn.k_proj.bias', 'audio_encoder.encoder.layers.6.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.6.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.6.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.6.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.6.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.6.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.6.self_attn.grep_linear.weight', 'audio_encoder.encoder.layers.6.self_attn.grep_linear.bias', 'audio_encoder.encoder.layers.6.self_attn.relative_attention_bias.weight', 'audio_encoder.encoder.layers.6.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.6.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.6.fc1.weight', 'audio_encoder.encoder.layers.6.fc1.bias', 'audio_encoder.encoder.layers.6.fc2.weight', 'audio_encoder.encoder.layers.6.fc2.bias', 'audio_encoder.encoder.layers.6.final_layer_norm.weight', 'audio_encoder.encoder.layers.6.final_layer_norm.bias', 'audio_encoder.encoder.layers.7.self_attn.grep_a', 'audio_encoder.encoder.layers.7.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.7.self_attn.k_proj.bias', 'audio_encoder.encoder.layers.7.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.7.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.7.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.7.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.7.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.7.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.7.self_attn.grep_linear.weight', 'audio_encoder.encoder.layers.7.self_attn.grep_linear.bias', 'audio_encoder.encoder.layers.7.self_attn.relative_attention_bias.weight', 'audio_encoder.encoder.layers.7.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.7.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.7.fc1.weight', 'audio_encoder.encoder.layers.7.fc1.bias', 'audio_encoder.encoder.layers.7.fc2.weight', 'audio_encoder.encoder.layers.7.fc2.bias', 'audio_encoder.encoder.layers.7.final_layer_norm.weight', 'audio_encoder.encoder.layers.7.final_layer_norm.bias', 'audio_encoder.encoder.layers.8.self_attn.grep_a', 'audio_encoder.encoder.layers.8.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.8.self_attn.k_proj.bias', 'audio_encoder.encoder.layers.8.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.8.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.8.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.8.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.8.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.8.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.8.self_attn.grep_linear.weight', 'audio_encoder.encoder.layers.8.self_attn.grep_linear.bias', 'audio_encoder.encoder.layers.8.self_attn.relative_attention_bias.weight', 'audio_encoder.encoder.layers.8.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.8.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.8.fc1.weight', 'audio_encoder.encoder.layers.8.fc1.bias', 'audio_encoder.encoder.layers.8.fc2.weight', 'audio_encoder.encoder.layers.8.fc2.bias', 'audio_encoder.encoder.layers.8.final_layer_norm.weight', 'audio_encoder.encoder.layers.8.final_layer_norm.bias', 'audio_encoder.encoder.layers.9.self_attn.grep_a', 'audio_encoder.encoder.layers.9.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.9.self_attn.k_proj.bias', 'audio_encoder.encoder.layers.9.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.9.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.9.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.9.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.9.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.9.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.9.self_attn.grep_linear.weight', 'audio_encoder.encoder.layers.9.self_attn.grep_linear.bias', 'audio_encoder.encoder.layers.9.self_attn.relative_attention_bias.weight', 'audio_encoder.encoder.layers.9.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.9.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.9.fc1.weight', 'audio_encoder.encoder.layers.9.fc1.bias', 'audio_encoder.encoder.layers.9.fc2.weight', 'audio_encoder.encoder.layers.9.fc2.bias', 'audio_encoder.encoder.layers.9.final_layer_norm.weight', 'audio_encoder.encoder.layers.9.final_layer_norm.bias', 'audio_encoder.encoder.layers.10.self_attn.grep_a', 'audio_encoder.encoder.layers.10.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.10.self_attn.k_proj.bias', 'audio_encoder.encoder.layers.10.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.10.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.10.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.10.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.10.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.10.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.10.self_attn.grep_linear.weight', 'audio_encoder.encoder.layers.10.self_attn.grep_linear.bias', 'audio_encoder.encoder.layers.10.self_attn.relative_attention_bias.weight', 'audio_encoder.encoder.layers.10.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.10.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.10.fc1.weight', 'audio_encoder.encoder.layers.10.fc1.bias', 'audio_encoder.encoder.layers.10.fc2.weight', 'audio_encoder.encoder.layers.10.fc2.bias', 'audio_encoder.encoder.layers.10.final_layer_norm.weight', 'audio_encoder.encoder.layers.10.final_layer_norm.bias', 'audio_encoder.encoder.layers.11.self_attn.grep_a', 'audio_encoder.encoder.layers.11.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.11.self_attn.k_proj.bias', 'audio_encoder.encoder.layers.11.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.11.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.11.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.11.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.11.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.11.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.11.self_attn.grep_linear.weight', 'audio_encoder.encoder.layers.11.self_attn.grep_linear.bias', 'audio_encoder.encoder.layers.11.self_attn.relative_attention_bias.weight', 'audio_encoder.encoder.layers.11.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.11.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.11.fc1.weight', 'audio_encoder.encoder.layers.11.fc1.bias', 'audio_encoder.encoder.layers.11.fc2.weight', 'audio_encoder.encoder.layers.11.fc2.bias', 'audio_encoder.encoder.layers.11.final_layer_norm.weight', 'audio_encoder.encoder.layers.11.final_layer_norm.bias', 'audio_encoder.encoder.layer_norm.weight', 'audio_encoder.encoder.layer_norm.bias', 'audio_encoder.layer_norm.weight', 'audio_encoder.layer_norm.bias', 'audio_proj.weight', 'audio_proj.bias', 'av_concat_audio_proj.0.weight', 'av_concat_audio_proj.0.bias', 'av_concat_audio_proj.1.weight', 'av_concat_audio_proj.1.bias', 'av_fusion.weight', 'av_fusion.bias', 'atm_head.weight', 'atm_head.bias', 'avtm_head.weight', 'avtm_head.bias', 'itm_head.weight', 'itm_head.bias'])\n",
      "Bfloat 16 mode\n"
     ]
    }
   ],
   "source": [
    "from models.internvideo2_stage2 import InternVideo2_Stage2\n",
    "from models.internvideo2_clip import InternVideo2_CLIP\n",
    "intern_model, tokenizer = setup_internvideo2(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from iv2_utils.iv2 import *\n",
    "\n",
    "if 'photography-model' not in os.listdir('.'):\n",
    "    run('git clone https://github.com/ruo2019/photography-model.git')\n",
    "phrases = [x[1][1] for x in json_read('photography-model/data/ACT75.json')]\n",
    "\n",
    "video = cv2.VideoCapture('photography-model/data/act75/1.mp4')\n",
    "frame1 = [x for x in _frame_from_video(video)]\n",
    "frame1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "intern_model = intern_model.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/189 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "  0%|          | 0/189 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected mat1 and mat2 to have the same dtype, but got: float != c10::BFloat16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(frames) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m---> 19\u001b[0m     texts, probs \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m:\u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintern_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     logit_curr\u001b[38;5;241m.\u001b[39mappend(probs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(logit_curr) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/IV2/InternVideo2/multi_modality/demo/utils.py:91\u001b[0m, in \u001b[0;36mretrieve_text\u001b[0;34m(frames, texts, model, topk, config, device, log)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m log: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Cached\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m     text_feats_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([tensor_cache[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m texts])\n\u001b[0;32m---> 91\u001b[0m probs, idxs \u001b[38;5;241m=\u001b[39m \u001b[43mvlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvid_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_feats_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# print(\"-\" * 30)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# print(probs)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# print(idxs)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# print(\"-\" * 30)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m ret_texts \u001b[38;5;241m=\u001b[39m [texts[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idxs\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()]\n",
      "File \u001b[0;32m~/IV2/InternVideo2/multi_modality/demo/utils.py:339\u001b[0m, in \u001b[0;36mInternVideo2_Stage2.predict_label\u001b[0;34m(self, vid_feat, txt_feat, top)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_label\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    336\u001b[0m                   vid_feat: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    337\u001b[0m                   txt_feat: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    338\u001b[0m                   top: \u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m--> 339\u001b[0m     label_probs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvid_feat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtxt_feat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m)\n\u001b[1;32m    340\u001b[0m     top_probs, top_labels \u001b[38;5;241m=\u001b[39m label_probs\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtopk(top, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m top_probs, top_labels\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected mat1 and mat2 to have the same dtype, but got: float != c10::BFloat16"
     ]
    }
   ],
   "source": [
    "# ORIGINAL\n",
    "from tqdm import tqdm\n",
    "\n",
    "TOKEN = \"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\"\n",
    "\n",
    "MODEL_NAME = \"6B\"\n",
    "\n",
    "output_dir = 'output'\n",
    "\n",
    "logits = []\n",
    "preds  = []\n",
    "for video_path, phrase, frames in json_read('photography-model/data/ACT75.json'):\n",
    "    frames = [x for x in _frame_from_video(cv2.VideoCapture('photography-model/' + video_path))]\n",
    "\n",
    "    logit_curr = []\n",
    "    pbar = tqdm(range(len(frames) - 3))\n",
    "\n",
    "    for j in pbar:\n",
    "        texts, probs = retrieve_text(frames[j:j+4], [phrase], model=intern_model, topk=1, config=config)\n",
    "        logit_curr.append(probs[0])\n",
    "        if len(logit_curr) > 0:\n",
    "            pbar.set_description(str(np.argmax(logit_curr) + 1))\n",
    "    preds.append(np.argmax(logit_curr) + 1)\n",
    "    print(list(zip(logit_curr, range(1, len(logit_curr) + 1))))\n",
    "    logits.append(list(zip(logit_curr, range(1, len(logit_curr) + 1))))\n",
    "\n",
    "print(preds[:5])\n",
    "preds = [int(x) for x in preds]\n",
    "\n",
    "logits_v2 = [[(float(l[0]), l[1]) for l in x] for x in logits]\n",
    "\n",
    "json_write(preds, f'ACT75-V5-InternVideo-{MODEL_NAME}-t4.json')\n",
    "json_write(logits_v2, f'ACT75-V5-InternVideo-{MODEL_NAME}-logits-t.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
