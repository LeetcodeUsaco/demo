{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/root/IV2/InternVideo2/multi_modality/models/backbones/internvideo2/internvl_clip_vision.py:140: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n",
      "/root/IV2/InternVideo2/multi_modality/models/backbones/internvideo2/internvideo2.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n",
      "/usr/local/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/root/IV2/InternVideo2/multi_modality/models/backbones/internvideo2/internvideo2_clip_vision.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'deepspeed'\n",
      "deepspeed is not installed!!!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "\n",
    "from demo.config import (Config,\n",
    "                    eval_dict_leaf)\n",
    "\n",
    "from demo.utils import (retrieve_text,\n",
    "                  _frame_from_video,\n",
    "                  setup_internvideo2)\n",
    "\n",
    "from iv2_utils.iv2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from transformers import AutoTokenizer\n",
    "from models.backbones.bert.tokenization_bert import BertTokenizer\n",
    "\n",
    "# bert_path = snapshot_download(repo_id=\"google-bert/bert-large-uncased\")\n",
    "\n",
    "vision_ckpt_path = hf_hub_download(repo_id=\"OpenGVLab/InternVideo2-Stage2_6B-224p-f4\", filename=\"internvideo2-s2_6b-224p-f4.pt\")\n",
    "\n",
    "extra_ckpt = hf_hub_download(repo_id=\"OpenGVLab/InternVideo2-CLIP-1B-224p-f8\", filename=\"1B_clip.pth\")\n",
    "\n",
    "# bert_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-large-uncased\")#, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config.from_file('scripts/pretraining/stage2/6B/config.py')\n",
    "config = eval_dict_leaf(config)\n",
    "# config.device = 'mps'\n",
    "\n",
    "# config.model.text_ckpt_path = 'model/internvl_c_13b_224px.pth'\n",
    "# config.model.vision_ckpt_path = vision_ckpt_path\n",
    "config.model.vision_encoder.pretrained = vision_ckpt_path\n",
    "config.pretrained_path = vision_ckpt_path\n",
    "# config.model.tokenizer_path = bert_path\n",
    "# config.model.text_encoder.llama_path = 'model/chinese_alpaca_lora_7b'\n",
    "# config.model.extra_ckpt_path = extra_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.internvideo2_stage2 import InternVideo2_Stage2\n",
    "from models.internvideo2_clip import InternVideo2_CLIP\n",
    "intern_model, tokenizer = setup_internvideo2(config)\n",
    "intern_model = intern_model.eval()\n",
    "# intern_model = intern_model.to_empty(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture('demo/example1.mp4')\n",
    "frames = [x for x in _frame_from_video(video)]\n",
    "\n",
    "phrases = [\"A playful dog and its owner wrestle in the snowy yard, chasing each other with joyous abandon.\",\n",
    "           \"A helicopter lands on a building.\",\n",
    "           \"A person bundled up in a blanket walks through the snowy landscape, enjoying the serene winter scenery.\"]\n",
    "\n",
    "texts, probs = retrieve_text(frames, phrases, model=intern_model, topk=len(phrases), config=config)\n",
    "\n",
    "for t, p in zip(texts, probs):\n",
    "    print(f'text: {t} ~ prob: {p:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo.utils import frames2tensor\n",
    "\n",
    "fn = config.get('num_frames', 8)\n",
    "size_t = config.get('size_t', 224)\n",
    "frames_tensor = frames2tensor(frames, fnum=fn, target_size=(size_t, size_t), device=torch.device('cuda'))\n",
    "vid_feat = intern_model.get_vid_feat(frames_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intern_model.get_txt_feat('Example of text embedding').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "total_params = sum(p.numel() for p in intern_model.parameters())\n",
    "\n",
    "print(f'InternVideo 2 Vision Encoder has: {total_params:,} parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import system as run\n",
    "from iv2_utils.iv2 import *\n",
    "\n",
    "if 'photography-model' not in os.listdir('.'):\n",
    "    run('git clone https://github.com/ruo2019/photography-model.git')\n",
    "phrases = [x[1][1] for x in json_read('photography-model/rustyjar/ACT75.json')]\n",
    "\n",
    "video = cv2.VideoCapture('photography-model/ACT75/1.mp4')\n",
    "frame1 = [x for x in _frame_from_video(video)]\n",
    "frame1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_dir = 'output'\n",
    "logits = []\n",
    "preds  = []\n",
    "for video_path, phrase, frames in json_read('photography-model/rustyjar/ACT75.json'):\n",
    "    frames = [x for x in _frame_from_video(cv2.VideoCapture('photography-model/' + video_path))]\n",
    "\n",
    "    logit_curr = []\n",
    "    pbar = tqdm(range(len(frames) - 7))\n",
    "\n",
    "    for j in pbar:\n",
    "        texts, probs = retrieve_text(frames[j:j+8], [phrase], model=intern_model, topk=1, config=config)\n",
    "        logit_curr.append(probs[0])\n",
    "        if len(logit_curr) > 0:\n",
    "            pbar.set_description(str(np.argmax(logit_curr) + 1))\n",
    "    preds.append(np.argmax(logit_curr) + 1)\n",
    "    logits.append(list(zip(logit_curr, range(1, len(logit_curr) + 1))))\n",
    "\n",
    "print(preds[:5])\n",
    "preds = [int(x) for x in preds]\n",
    "\n",
    "logits_v2 = [[(float(l[0]), l[1]) for l in x] for x in logits]\n",
    "\n",
    "json_write(preds, f'InternVideo-6B-t8.json')\n",
    "json_write(logits_v2, f'InternVideo-6B-logits-t.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "TOKEN = \"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\"\n",
    "\n",
    "login(TOKEN)\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=f'InternVideo-6B-t8.json',\n",
    "    path_in_repo=f'InternVideo-6B-t8.json',\n",
    "    repo_id=\"qingy2024/InternVideo2-Data\",\n",
    "    repo_type=\"dataset\",\n",
    ")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=f'InternVideo-6B-logits-t.json',\n",
    "    path_in_repo=f'InternVideo-6B-logits-t.json',\n",
    "    repo_id=\"qingy2024/InternVideo2-Data\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
